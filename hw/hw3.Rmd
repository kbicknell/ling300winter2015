---
title: 'Ling 300: Homework 3'
author: "Instructor: Klinton Bicknell"
date: "Due: February 17, 2015"
output: html_document
---

Unlike the previous homework assignments, this one does not have separate short answer questions that are unrelated to coding. Thus, you will primarily turn in your R code. However, some components do also include short answer questions. These are marked by the word __Question__ in bold. You can submit these answers in a separate pdf OR answer them in the comments of your R script at the appropriate positions.

Bayesian inference with JAGS
----------------------------

For this assignment, we'll make use of a dataset of lexical decision responses. As you may be aware, lexical decision is a task in which participants see a string of characters such as 'flutter' or 'blutcher' and have to rapidly push a button indicating whether it is a real word or not. Your dataset is the set of responses ('word' or 'nonword') made to actual words by 17 participants and is available as `bicknell_et_al_2010_lexdec_ling300.csv` [download here](bicknell_et_al_2010_lexdec_ling300.csv).

In particular, we're interested in using the binary answers (correct / incorrect) to make inferences about the probability of getting a trial correct in this task, which we'll call $\pi$. (Remember that 'correct' means responding 'word', since this dataset is all actual words.) We'll use two different Bayesian methods to estimate the posterior distribution on $\pi$. Note that by itself, of course, this probability isn't very interesting. However, these models are a simpler piece of an interesting analysis that examining whether $\pi$ differs across conditions (for example, following related or unrelated prime words).

1. First steps:

    * As the first R commands in your script, load the packages you'll need for this assignment (`dplyr`, `ggplot2`, `rjags`)
    * Load the data from the file to a data frame called `dat`. (Do not use `file.choose()`.)
    * Note that the second variable is factor with two levels 'word' and 'nonword'. For the use of this variable with JAGS, it would be more convenient if it was a numeric vector, where a value of 1 indicates correct and 0 indicates incorrect. Using the `dplyr` package, add a column to this data frame called `correct` that has this property.
    
### Part 1: simple analysis of posterior ###
To start analyzing our beliefs about $\pi$, we'll use a very simple model, much like the one we discussed in class, and then perform inference with JAGS.

2. To get a sense of the overall data, plot a 95% bootstrapped confidence interval on the overall probability of responding correct. Since this is a single overall probability, not split by condition, the simplest way to get `ggplot` to do this is to specify a constant number for the x-axis aesthetic, e.g., `aes(1, correct)`. To get this type of interval out of `stat_summary`, use `mean_cl_boot` for the `fun.dat` argument instead of `mean_se` that you were using in the previous assignment. (Note that bootstrapped confidence intervals such as these are usually much better behaved for parameters bounded between 0 and 1 than the more common standard-error-based intervals.) __Questions:__ What are the endpoints of that interval? How much uncertainty does this (frequentist) analysis reveal about the probability of getting a trial correct (i.e., about how wide is the interval)?

3. The first step of creating this simple model is to create a variable called `model_string_simple` that describes this simple model in JAGS. The model should specify that the variable `pi` is distributed according to a uniform Beta distribution, and that each element of the variable `correct` is drawn from a Bernoulli trial with parameter `pi`.

4. The next step is to initialize variables named `num_samples_to_keep`, `num_samples_to_toss`, and `num_chains`. For now, keep just 50 samples, fix the number of samples to toss to be the same as the number of samples to keep, and use two Markov chains.

5. Now, define the initialization values for the two chains. The first should start at a value of `pi` at 0.01 and the other at 0.99. The first chain should use a random number seed of 13 and the second of 1871. Both chains should use the Mersenne Twister random number generator.

6. Next, initialize your JAGS model, using the model string, initialization values, and number of chains you've defined. You should also link the `correct` variable in your JAGS model to the `correct` variable in your data frame.

7. Run burn-in for `num_samples_to_toss` iterations.

8. Now, get your samples from the model, specifying that `pi` is the variable for which we are interested in obtaining samples. Store your samples in a variable named `mcmc_samples_simple`.

9. As is usually the first thing you should do to check mcmc output, run `gelman.diag` on your samples. __Questions:__ Is the Upper C.I. of $\hat{R}$ for your variable of interest below 1.05 as desired?

10. Another useful thing to check is how many effectively independent samples from the posterior the mcmc samples contain about a given variable of interest. Check that from this mcmc output with the `effectiveSize` function. If a variable is mixing very well, then practically every mcmc iterate will be an independent sample from the posterior, and thus the number of effective samples per iterate will be near 1. Conversely, if a variable is mixing poorly, this ratio could be very close to zero. __Question:__ Is `pi` mixing well or poorly in this model? Justify your answer.

11. Finally, we'll extract an estimate for the posterior mean and estimates for the bounds of the posterior 95% central interval from our samples. To do this, we'll first need to change our mcmc samples object into a simple vector of samples. To do this, first convert the mcmc samples to a matrix using the `as.matrix()` function. Then, convert that matrix into a data frame with the `as.data.frame()` function. Now, extract the `pi` column from that data frame into a vector called `samples_vector_simple`. Using this vector:
    * Estimate the mean of the posterior by taking the mean of these samples.
    * Estimate the bounds of the 95% central interval of the posterior by taking the 0.025 and 0.975 quantiles of these samples. You can do this using the `quantile()` function.
    * __Questions:__ What are the estimated posterior mean and 95% central interval? Exactly how wide is that interval estimated to be? How well do these numbers agree with the mean and bounds that you plotted with bootstrap methods above?
    
### Part 2: more complex analysis of posterior ###
    
    [TBA]